<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Anthropic Alignment Science - AI-PULSE</title><link>https://alignment.anthropic.com</link><description>AI safety research and alignment science from Anthropic</description><docs>http://www.rssboard.org/rss-specification</docs><generator>AI-PULSE RSS Generator</generator><language>en</language><lastBuildDate>Mon, 30 Jun 2025 08:31:55 +0000</lastBuildDate><item><title>This blog is inspired by the informal updates on our
                Interpretability team'sTransformer Circuits thread. We'll use it to release
                research notes and early findings
                that we don't think warrant a full publication, but might
                nonetheless be useful to others working on similar problems.</title><link>https://transformer-circuits.pub/</link><description>AI Safety research from Anthropic: This blog is inspired by the informal updates on our
                Interpretability team'sTransformer Circuits thread. We'll use it to release
                research notes and early findings
                that we don't think warrant a full publication, but might
                nonetheless be useful to others working on similar problems.</description><guid isPermaLink="false">https://transformer-circuits.pub/</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Alignment Science team</title><link>https://www.anthropic.com/research#alignment</link><description>AI Safety research from Anthropic: Alignment Science team</description><guid isPermaLink="false">https://www.anthropic.com/research#alignment</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>We are Anthropic'sAlignment Science team. We do machine
                learning research on the problem of
                steering and controlling future powerful AI systems, as well
                as understanding and evaluating the risks that they pose.
                Welcome to our blog!</title><link>https://www.anthropic.com/research#alignment</link><description>AI Safety research from Anthropic: We are Anthropic'sAlignment Science team. We do machine
                learning research on the problem of
                steering and controlling future powerful AI systems, as well
                as understanding and evaluating the risks that they pose.
                Welcome to our blog!</description><guid isPermaLink="false">https://www.anthropic.com/research#alignment</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Discovering Language Model Behaviors with Model-Written
                    Evaluations</title><link>https://www.anthropic.com/research/discovering-language-model-behaviors-with-model-written-evaluations</link><description>AI Safety research from Anthropic: Discovering Language Model Behaviors with Model-Written
                    Evaluations</description><guid isPermaLink="false">https://www.anthropic.com/research/discovering-language-model-behaviors-with-model-written-evaluations</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Sleeper Agents: Training Deceptive LLMs that Persist
                    Through Safety Training</title><link>https://www.anthropic.com/research/many-shot-jailbreaking</link><description>AI Safety research from Anthropic: Sleeper Agents: Training Deceptive LLMs that Persist
                    Through Safety Training</description><guid isPermaLink="false">https://www.anthropic.com/research/many-shot-jailbreaking</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Simple Probes can Catch Sleeper Agents</title><link>https://www.anthropic.com/research/probes-catch-sleeper-agents</link><description>AI Safety research from Anthropic: Simple Probes can Catch Sleeper Agents</description><guid isPermaLink="false">https://www.anthropic.com/research/probes-catch-sleeper-agents</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Sabotage Evaluations for Frontier Models</title><link>https://www.anthropic.com/research/sabotage-evaluations</link><description>AI Safety research from Anthropic: Sabotage Evaluations for Frontier Models</description><guid isPermaLink="false">https://www.anthropic.com/research/sabotage-evaluations</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Three Sketches of ASL-4 Safety Case Components</title><link>https://alignment.anthropic.com/2024/safety-cases/index.html</link><description>AI Safety research from Anthropic: Three Sketches of ASL-4 Safety Case Components</description><guid isPermaLink="false">https://alignment.anthropic.com/2024/safety-cases/index.html</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Introducing the Anthropic Fellows Program for AI Safety
                    Research</title><link>https://alignment.anthropic.com/2024/anthropic-fellows-program/index.html</link><description>AI Safety research from Anthropic: Introducing the Anthropic Fellows Program for AI Safety
                    Research</description><guid isPermaLink="false">https://alignment.anthropic.com/2024/anthropic-fellows-program/index.html</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>A Toy Evaluation of Inference Code Tampering</title><link>https://alignment.anthropic.com/2024/rogue-eval/index.html</link><description>AI Safety research from Anthropic: A Toy Evaluation of Inference Code Tampering</description><guid isPermaLink="false">https://alignment.anthropic.com/2024/rogue-eval/index.html</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>How to Replicate and Extend our Alignment Faking Demo</title><link>https://alignment.anthropic.com/2024/how-to-alignment-faking/index.html</link><description>AI Safety research from Anthropic: How to Replicate and Extend our Alignment Faking Demo</description><guid isPermaLink="false">https://alignment.anthropic.com/2024/how-to-alignment-faking/index.html</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Alignment Faking in Large Language Models</title><link>https://www.anthropic.com/research/alignment-faking</link><description>AI Safety research from Anthropic: Alignment Faking in Large Language Models</description><guid isPermaLink="false">https://www.anthropic.com/research/alignment-faking</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Recommendations for Technical AI Safety Research
                    Directions</title><link>https://alignment.anthropic.com/2025/recommended-directions/index.html</link><description>AI Safety research from Anthropic: Recommendations for Technical AI Safety Research
                    Directions</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/recommended-directions/index.html</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Forecasting Rare Language Model Behaviors</title><link>https://www.anthropic.com/research/forecasting-rare-behaviors</link><description>AI Safety research from Anthropic: Forecasting Rare Language Model Behaviors</description><guid isPermaLink="false">https://www.anthropic.com/research/forecasting-rare-behaviors</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Monitoring Computer Use via Hierarchical Summarization</title><link>https://alignment.anthropic.com/2025/summarization-for-monitoring/index.html</link><description>AI Safety research from Anthropic: Monitoring Computer Use via Hierarchical Summarization</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/summarization-for-monitoring/index.html</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Won't vs. Can't: Sandbagging-like Behavior from Claude Models</title><link>https://alignment.anthropic.com/2025/wont-vs-cant/</link><description>AI Safety research from Anthropic: Won't vs. Can't: Sandbagging-like Behavior from Claude Models</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/wont-vs-cant/</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Introducing Anthropic's Safeguards Research Team</title><link>https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html</link><description>AI Safety research from Anthropic: Introducing Anthropic's Safeguards Research Team</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Automated Researchers Can Subtly Sandbag</title><link>https://alignment.anthropic.com/2025/automated-researchers-sandbag/</link><description>AI Safety research from Anthropic: Automated Researchers Can Subtly Sandbag</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/automated-researchers-sandbag/</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Auditing Language Models for Hidden Objectives</title><link>https://www.anthropic.com/research/auditing-hidden-objectives</link><description>AI Safety research from Anthropic: Auditing Language Models for Hidden Objectives</description><guid isPermaLink="false">https://www.anthropic.com/research/auditing-hidden-objectives</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Alignment Faking Revisited: Improved Classifiers and Open Source Extensions</title><link>https://alignment.anthropic.com/2025/alignment-faking-revisited/</link><description>AI Safety research from Anthropic: Alignment Faking Revisited: Improved Classifiers and Open Source Extensions</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/alignment-faking-revisited/</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Putting up Bumpers</title><link>https://alignment.anthropic.com/2025/bumpers/</link><description>AI Safety research from Anthropic: Putting up Bumpers</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/bumpers/</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Modifying LLM Beliefs with Synthetic Document Finetuning</title><link>https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf/</link><description>AI Safety research from Anthropic: Modifying LLM Beliefs with Synthetic Document Finetuning</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf/</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Publicly Releasing CoT Faithfulness Evaluations</title><link>https://drive.google.com/drive/folders/1l0pkcZxvFwMtczst_hhiCC44v-IiODlY?usp=sharing</link><description>AI Safety research from Anthropic: Publicly Releasing CoT Faithfulness Evaluations</description><guid isPermaLink="false">https://drive.google.com/drive/folders/1l0pkcZxvFwMtczst_hhiCC44v-IiODlY?usp=sharing</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Reasoning Models Don't Always Say What They Think</title><link>https://www.anthropic.com/research/reasoning-models-dont-say-think</link><description>AI Safety research from Anthropic: Reasoning Models Don't Always Say What They Think</description><guid isPermaLink="false">https://www.anthropic.com/research/reasoning-models-dont-say-think</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item><item><title>Model-Internals Classifiers</title><link>https://alignment.anthropic.com/2025/cheap-monitors/</link><description>AI Safety research from Anthropic: Model-Internals Classifiers</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/cheap-monitors/</guid><category>Alignment Science</category><pubDate>Mon, 30 Jun 2025 08:31:55 +0000</pubDate></item></channel></rss>