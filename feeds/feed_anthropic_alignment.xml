<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Anthropic Alignment Science - AI-PULSE</title><link>https://alignment.anthropic.com</link><description>AI safety research and alignment science from Anthropic</description><docs>http://www.rssboard.org/rss-specification</docs><generator>AI-PULSE RSS Generator</generator><language>en</language><lastBuildDate>Fri, 29 Aug 2025 01:43:32 +0000</lastBuildDate><item><title>Alignment Science Blog</title><link>https://alignment.anthropic.com/2025/openai-findings/</link><description>AI Safety research: Anthropic and OpenAI conducted simultaneous alignment assessments of each others' models earlier
                    thi...</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/openai-findings/</guid><category>Alignment Science</category><pubDate>Fri, 29 Aug 2025 01:43:32 +0000</pubDate></item><item><title>Discovering Language Model Behaviors with Model-Written
                    Evaluations</title><link>https://www.anthropic.com/research/discovering-language-model-behaviors-with-model-written-evaluations</link><description>AI Safety research: We develop an automated way to generate language model
                    (LM) evaluations with LMs, significantly redu...</description><guid isPermaLink="false">https://www.anthropic.com/research/discovering-language-model-behaviors-with-model-written-evaluations</guid><category>Alignment Science</category><pubDate>Fri, 29 Aug 2025 01:43:32 +0000</pubDate></item><item><title>Sleeper Agents: Training Deceptive LLMs that Persist
                    Through Safety Training</title><link>https://www.anthropic.com/research/many-shot-jailbreaking</link><description>AI Safety research: We train LLMs to act secretly malicious. We find that,
                    despite our best efforts at alignment trainin...</description><guid isPermaLink="false">https://www.anthropic.com/research/many-shot-jailbreaking</guid><category>Alignment Science</category><pubDate>Fri, 29 Aug 2025 01:43:32 +0000</pubDate></item><item><title>Simple Probes can Catch Sleeper Agents</title><link>https://www.anthropic.com/research/probes-catch-sleeper-agents</link><description>AI Safety research: We find that probing, a simple interpretability
                    technique, can detect when backdoored "sleeper agent...</description><guid isPermaLink="false">https://www.anthropic.com/research/probes-catch-sleeper-agents</guid><category>Alignment Science</category><pubDate>Fri, 29 Aug 2025 01:43:32 +0000</pubDate></item><item><title>Sabotage Evaluations for Frontier Models</title><link>https://www.anthropic.com/research/sabotage-evaluations</link><description>AI Safety research: How well could AI models mislead us, or secretly
                    sabotage tasks, if they were trying to? We describe...</description><guid isPermaLink="false">https://www.anthropic.com/research/sabotage-evaluations</guid><category>Alignment Science</category><pubDate>Fri, 29 Aug 2025 01:43:32 +0000</pubDate></item><item><title>How to Replicate and Extend our Alignment Faking Demo</title><link>https://alignment.anthropic.com/2024/how-to-alignment-faking/index.html</link><description>AI Safety research: We describe how to get started with experimenting with
                    our demonstration of alignment faking, and pr...</description><guid isPermaLink="false">https://alignment.anthropic.com/2024/how-to-alignment-faking/index.html</guid><category>Alignment Science</category><pubDate>Fri, 29 Aug 2025 01:43:32 +0000</pubDate></item><item><title>Alignment Faking in Large Language Models</title><link>https://www.anthropic.com/research/alignment-faking</link><description>AI Safety research: We present experiments where Claude often pretends to
                    have different views during training, while ac...</description><guid isPermaLink="false">https://www.anthropic.com/research/alignment-faking</guid><category>Alignment Science</category><pubDate>Fri, 29 Aug 2025 01:43:32 +0000</pubDate></item><item><title>Forecasting Rare Language Model Behaviors</title><link>https://www.anthropic.com/research/forecasting-rare-behaviors</link><description>AI Safety research: We forecast whether risks will occur after a model is deployed â€” using even very limited sets of
                    tes...</description><guid isPermaLink="false">https://www.anthropic.com/research/forecasting-rare-behaviors</guid><category>Alignment Science</category><pubDate>Fri, 29 Aug 2025 01:43:32 +0000</pubDate></item><item><title>Introducing Anthropic's Safeguards Research Team</title><link>https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html</link><description>AI Safety research: We're launching a new research team focused on mitigating the post-deployment risks of AI systems....</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html</guid><category>Alignment Science</category><pubDate>Fri, 29 Aug 2025 01:43:32 +0000</pubDate></item><item><title>Automated Researchers Can Subtly Sandbag</title><link>https://alignment.anthropic.com/2025/automated-researchers-sandbag/</link><description>AI Safety research: Current models can sandbag ML experiments and research decisions without being detected by zero-shot
                   ...</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/automated-researchers-sandbag/</guid><category>Alignment Science</category><pubDate>Fri, 29 Aug 2025 01:43:32 +0000</pubDate></item><item><title>Auditing Language Models for Hidden Objectives</title><link>https://www.anthropic.com/research/auditing-hidden-objectives</link><description>AI Safety research: We deliberately train a language model with a hidden objective and use it as a testbed for studying
                    ...</description><guid isPermaLink="false">https://www.anthropic.com/research/auditing-hidden-objectives</guid><category>Alignment Science</category><pubDate>Fri, 29 Aug 2025 01:43:32 +0000</pubDate></item><item><title>Alignment Faking Revisited: Improved Classifiers and Open Source Extensions</title><link>https://alignment.anthropic.com/2025/alignment-faking-revisited/</link><description>AI Safety research: We present a replication and extension of an alignment faking model organism....</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/alignment-faking-revisited/</guid><category>Alignment Science</category><pubDate>Fri, 29 Aug 2025 01:43:32 +0000</pubDate></item><item><title>Reasoning Models Don't Always Say What They Think</title><link>https://www.anthropic.com/research/reasoning-models-dont-say-think</link><description>AI Safety research: We find that reasoning models don't always accurately verbalize their reasoning. This casts doubt on
                   ...</description><guid isPermaLink="false">https://www.anthropic.com/research/reasoning-models-dont-say-think</guid><category>Alignment Science</category><pubDate>Fri, 29 Aug 2025 01:43:32 +0000</pubDate></item></channel></rss>