<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Anthropic Complete - AI-PULSE</title><link>https://www.anthropic.com</link><description>Complete feed: News, Engineering, and Alignment Science from Anthropic</description><docs>http://www.rssboard.org/rss-specification</docs><generator>AI-PULSE RSS Generator - Complete Feed</generator><language>en</language><lastBuildDate>Wed, 02 Jul 2025 01:53:51 +0000</lastBuildDate><item><title>[News] Press inquiries</title><link>https://www.anthropic.com/news/claude-4</link><description>Latest from Anthropic: Press inquiries (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/claude-4</guid><category>News</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[News] AnnouncementsIntroducing Claude 4</title><link>https://www.anthropic.com/news/claude-4</link><description>Latest from Anthropic: AnnouncementsIntroducing Claude 4 (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/claude-4</guid><category>News</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[News] AnnouncementsIntroducing Claude 4</title><link>https://www.anthropic.com/news/claude-4</link><description>Latest from Anthropic: AnnouncementsIntroducing Claude 4 (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/claude-4</guid><category>News</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[News] Press inquiries</title><link>mailto:press@anthropic.com</link><description>Latest from Anthropic: Press inquiries (Source: News)</description><guid isPermaLink="false">mailto:press@anthropic.com</guid><category>News</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[News] Press inquiries</title><link>mailto:press@anthropic.com</link><description>Latest from Anthropic: Press inquiries (Source: News)</description><guid isPermaLink="false">mailto:press@anthropic.com</guid><category>News</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[News] FeaturedAnthropic raises Series E at $61.5B post-money valuationFeaturedIntroducing the Anthropic Economic Index</title><link>https://www.anthropic.com/news/anthropic-raises-series-e-at-usd61-5b-post-money-valuation</link><description>Latest from Anthropic: FeaturedAnthropic raises Series E at $61.5B post-money valuationFeaturedIntroducing the Anthropic Economic Index (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/anthropic-raises-series-e-at-usd61-5b-post-money-valuation</guid><category>News</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[News] FeaturedAnthropic raises Series E at $61.5B post-money valuation</title><link>https://www.anthropic.com/news/anthropic-raises-series-e-at-usd61-5b-post-money-valuation</link><description>Latest from Anthropic: FeaturedAnthropic raises Series E at $61.5B post-money valuation (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/anthropic-raises-series-e-at-usd61-5b-post-money-valuation</guid><category>News</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[News] FeaturedAnthropic raises Series E at $61.5B post-money valuation</title><link>https://www.anthropic.com/news/anthropic-raises-series-e-at-usd61-5b-post-money-valuation</link><description>Latest from Anthropic: FeaturedAnthropic raises Series E at $61.5B post-money valuation (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/anthropic-raises-series-e-at-usd61-5b-post-money-valuation</guid><category>News</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[News] FeaturedIntroducing the Anthropic Economic Index</title><link>https://www.anthropic.com/news/the-anthropic-economic-index</link><description>Latest from Anthropic: FeaturedIntroducing the Anthropic Economic Index (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/the-anthropic-economic-index</guid><category>News</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[News] FeaturedIntroducing the Anthropic Economic Index</title><link>https://www.anthropic.com/news/the-anthropic-economic-index</link><description>Latest from Anthropic: FeaturedIntroducing the Anthropic Economic Index (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/the-anthropic-economic-index</guid><category>News</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[News] No results found.</title><link>https://www.anthropic.com/news/introducing-the-anthropic-economic-futures-program</link><description>Latest from Anthropic: No results found. (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/introducing-the-anthropic-economic-futures-program</guid><category>News</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[News] Introducing the Anthropic Economic Futures Program</title><link>https://www.anthropic.com/news/introducing-the-anthropic-economic-futures-program</link><description>Latest from Anthropic: Introducing the Anthropic Economic Futures Program (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/introducing-the-anthropic-economic-futures-program</guid><category>News</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[News] Introducing the Anthropic Economic Futures Program</title><link>https://www.anthropic.com/news/introducing-the-anthropic-economic-futures-program</link><description>Latest from Anthropic: Introducing the Anthropic Economic Futures Program (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/introducing-the-anthropic-economic-futures-program</guid><category>News</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[News] How People Use Claude for Support, Advice, and Companionship</title><link>https://www.anthropic.com/news/how-people-use-claude-for-support-advice-and-companionship</link><description>Latest from Anthropic: How People Use Claude for Support, Advice, and Companionship (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/how-people-use-claude-for-support-advice-and-companionship</guid><category>News</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[News] Build and share AI-powered apps with Claude</title><link>https://www.anthropic.com/news/claude-powered-artifacts</link><description>Latest from Anthropic: Build and share AI-powered apps with Claude (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/claude-powered-artifacts</guid><category>News</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[News] Turn ideas into interactive AI-powered apps</title><link>https://www.anthropic.com/news/build-artifacts</link><description>Latest from Anthropic: Turn ideas into interactive AI-powered apps (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/build-artifacts</guid><category>News</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Engineering] Desktop Extensions: One-click MCP server installation for Claude Desktop</title><link>https://www.anthropic.com/engineering/desktop-extensions</link><description>Engineering insights from Anthropic: Desktop Extensions: One-click MCP server installation for Claude Desktop (Source: Engineering)</description><guid isPermaLink="false">https://www.anthropic.com/engineering/desktop-extensions</guid><category>Engineering</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Engineering] Desktop Extensions: One-click MCP server installation for Claude Desktop</title><link>https://www.anthropic.com/engineering/desktop-extensions</link><description>Engineering insights from Anthropic: Desktop Extensions: One-click MCP server installation for Claude Desktop (Source: Engineering)</description><guid isPermaLink="false">https://www.anthropic.com/engineering/desktop-extensions</guid><category>Engineering</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Engineering] Desktop Extensions: One-click MCP server installation for Claude Desktop</title><link>https://www.anthropic.com/engineering/desktop-extensions</link><description>Engineering insights from Anthropic: Desktop Extensions: One-click MCP server installation for Claude Desktop (Source: Engineering)</description><guid isPermaLink="false">https://www.anthropic.com/engineering/desktop-extensions</guid><category>Engineering</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Engineering] How we built our multi-agent research system</title><link>https://www.anthropic.com/engineering/built-multi-agent-research-system</link><description>Engineering insights from Anthropic: How we built our multi-agent research system (Source: Engineering)</description><guid isPermaLink="false">https://www.anthropic.com/engineering/built-multi-agent-research-system</guid><category>Engineering</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Engineering] Claude Code: Best practices for agentic coding</title><link>https://www.anthropic.com/engineering/claude-code-best-practices</link><description>Engineering insights from Anthropic: Claude Code: Best practices for agentic coding (Source: Engineering)</description><guid isPermaLink="false">https://www.anthropic.com/engineering/claude-code-best-practices</guid><category>Engineering</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Model-Internals Classifiers</title><link>https://alignment.anthropic.com/2025/cheap-monitors/</link><description>AI Safety research from Anthropic: Model-Internals Classifiers (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/cheap-monitors/</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Reasoning Models Don't Always Say What They Think</title><link>https://www.anthropic.com/research/reasoning-models-dont-say-think</link><description>AI Safety research from Anthropic: Reasoning Models Don't Always Say What They Think (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research/reasoning-models-dont-say-think</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Publicly Releasing CoT Faithfulness Evaluations</title><link>https://drive.google.com/drive/folders/1l0pkcZxvFwMtczst_hhiCC44v-IiODlY?usp=sharing</link><description>AI Safety research from Anthropic: Publicly Releasing CoT Faithfulness Evaluations (Source: Alignment Science)</description><guid isPermaLink="false">https://drive.google.com/drive/folders/1l0pkcZxvFwMtczst_hhiCC44v-IiODlY?usp=sharing</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Modifying LLM Beliefs with Synthetic Document Finetuning</title><link>https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf/</link><description>AI Safety research from Anthropic: Modifying LLM Beliefs with Synthetic Document Finetuning (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf/</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Putting up Bumpers</title><link>https://alignment.anthropic.com/2025/bumpers/</link><description>AI Safety research from Anthropic: Putting up Bumpers (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/bumpers/</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Alignment Faking Revisited: Improved Classifiers and Open Source Extensions</title><link>https://alignment.anthropic.com/2025/alignment-faking-revisited/</link><description>AI Safety research from Anthropic: Alignment Faking Revisited: Improved Classifiers and Open Source Extensions (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/alignment-faking-revisited/</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Auditing Language Models for Hidden Objectives</title><link>https://www.anthropic.com/research/auditing-hidden-objectives</link><description>AI Safety research from Anthropic: Auditing Language Models for Hidden Objectives (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research/auditing-hidden-objectives</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Automated Researchers Can Subtly Sandbag</title><link>https://alignment.anthropic.com/2025/automated-researchers-sandbag/</link><description>AI Safety research from Anthropic: Automated Researchers Can Subtly Sandbag (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/automated-researchers-sandbag/</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Introducing Anthropic's Safeguards Research Team</title><link>https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html</link><description>AI Safety research from Anthropic: Introducing Anthropic's Safeguards Research Team (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Won't vs. Can't: Sandbagging-like Behavior from Claude Models</title><link>https://alignment.anthropic.com/2025/wont-vs-cant/</link><description>AI Safety research from Anthropic: Won't vs. Can't: Sandbagging-like Behavior from Claude Models (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/wont-vs-cant/</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Monitoring Computer Use via Hierarchical Summarization</title><link>https://alignment.anthropic.com/2025/summarization-for-monitoring/index.html</link><description>AI Safety research from Anthropic: Monitoring Computer Use via Hierarchical Summarization (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/summarization-for-monitoring/index.html</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Forecasting Rare Language Model Behaviors</title><link>https://www.anthropic.com/research/forecasting-rare-behaviors</link><description>AI Safety research from Anthropic: Forecasting Rare Language Model Behaviors (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research/forecasting-rare-behaviors</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Recommendations for Technical AI Safety Research
                    Directions</title><link>https://alignment.anthropic.com/2025/recommended-directions/index.html</link><description>AI Safety research from Anthropic: Recommendations for Technical AI Safety Research
                    Directions (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/recommended-directions/index.html</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Alignment Faking in Large Language Models</title><link>https://www.anthropic.com/research/alignment-faking</link><description>AI Safety research from Anthropic: Alignment Faking in Large Language Models (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research/alignment-faking</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] How to Replicate and Extend our Alignment Faking Demo</title><link>https://alignment.anthropic.com/2024/how-to-alignment-faking/index.html</link><description>AI Safety research from Anthropic: How to Replicate and Extend our Alignment Faking Demo (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2024/how-to-alignment-faking/index.html</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] A Toy Evaluation of Inference Code Tampering</title><link>https://alignment.anthropic.com/2024/rogue-eval/index.html</link><description>AI Safety research from Anthropic: A Toy Evaluation of Inference Code Tampering (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2024/rogue-eval/index.html</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Introducing the Anthropic Fellows Program for AI Safety
                    Research</title><link>https://alignment.anthropic.com/2024/anthropic-fellows-program/index.html</link><description>AI Safety research from Anthropic: Introducing the Anthropic Fellows Program for AI Safety
                    Research (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2024/anthropic-fellows-program/index.html</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Three Sketches of ASL-4 Safety Case Components</title><link>https://alignment.anthropic.com/2024/safety-cases/index.html</link><description>AI Safety research from Anthropic: Three Sketches of ASL-4 Safety Case Components (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2024/safety-cases/index.html</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Sabotage Evaluations for Frontier Models</title><link>https://www.anthropic.com/research/sabotage-evaluations</link><description>AI Safety research from Anthropic: Sabotage Evaluations for Frontier Models (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research/sabotage-evaluations</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Simple Probes can Catch Sleeper Agents</title><link>https://www.anthropic.com/research/probes-catch-sleeper-agents</link><description>AI Safety research from Anthropic: Simple Probes can Catch Sleeper Agents (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research/probes-catch-sleeper-agents</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Sleeper Agents: Training Deceptive LLMs that Persist
                    Through Safety Training</title><link>https://www.anthropic.com/research/many-shot-jailbreaking</link><description>AI Safety research from Anthropic: Sleeper Agents: Training Deceptive LLMs that Persist
                    Through Safety Training (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research/many-shot-jailbreaking</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Discovering Language Model Behaviors with Model-Written
                    Evaluations</title><link>https://www.anthropic.com/research/discovering-language-model-behaviors-with-model-written-evaluations</link><description>AI Safety research from Anthropic: Discovering Language Model Behaviors with Model-Written
                    Evaluations (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research/discovering-language-model-behaviors-with-model-written-evaluations</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] We are Anthropic'sAlignment Science team. We do machine
                learning research on the problem of
                steering and controlling future powerful AI systems, as well
                as understanding and evaluating the risks that they pose.
                Welcome to our blog!</title><link>https://www.anthropic.com/research#alignment</link><description>AI Safety research from Anthropic: We are Anthropic'sAlignment Science team. We do machine
                learning research on the problem of
                steering and controlling future powerful AI systems, as well
                as understanding and evaluating the risks that they pose.
                Welcome to our blog! (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research#alignment</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] Alignment Science team</title><link>https://www.anthropic.com/research#alignment</link><description>AI Safety research from Anthropic: Alignment Science team (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research#alignment</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item><item><title>[Alignment Science] This blog is inspired by the informal updates on our
                Interpretability team'sTransformer Circuits thread. We'll use it to release
                research notes and early findings
                that we don't think warrant a full publication, but might
                nonetheless be useful to others working on similar problems.</title><link>https://transformer-circuits.pub/</link><description>AI Safety research from Anthropic: This blog is inspired by the informal updates on our
                Interpretability team'sTransformer Circuits thread. We'll use it to release
                research notes and early findings
                that we don't think warrant a full publication, but might
                nonetheless be useful to others working on similar problems. (Source: Alignment Science)</description><guid isPermaLink="false">https://transformer-circuits.pub/</guid><category>Alignment Science</category><pubDate>Wed, 02 Jul 2025 01:53:51 +0000</pubDate></item></channel></rss>