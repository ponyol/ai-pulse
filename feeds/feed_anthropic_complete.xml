<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Anthropic Complete - AI-PULSE</title><link>https://www.anthropic.com</link><description>Complete feed: News, Engineering, and Alignment Science from Anthropic</description><docs>http://www.rssboard.org/rss-specification</docs><generator>AI-PULSE RSS Generator - Complete Feed</generator><language>en</language><lastBuildDate>Thu, 30 Oct 2025 01:50:43 +0000</lastBuildDate><item><title>[News] Introducing Claude Sonnet 4.5</title><link>https://www.anthropic.com/news/claude-sonnet-4-5</link><description>Latest from Anthropic: Introducing Claude Sonnet 4.5 (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/claude-sonnet-4-5</guid><category>News</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[News] Thoughts on America’s AI Action Plan</title><link>https://www.anthropic.com/news/thoughts-on-america-s-ai-action-plan</link><description>Latest from Anthropic: Thoughts on America’s AI Action Plan (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/thoughts-on-america-s-ai-action-plan</guid><category>News</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[News] Anthropic raises $13B Series F at $183B post-money valuation</title><link>https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation</link><description>Latest from Anthropic: Anthropic raises $13B Series F at $183B post-money valuation (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation</guid><category>News</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[News] Anthropic Economic Index report: Uneven geographic and enterprise AI adoption</title><link>https://www.anthropic.com/research/anthropic-economic-index-september-2025-report</link><description>Latest from Anthropic: Anthropic Economic Index report: Uneven geographic and enterprise AI adoption (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/research/anthropic-economic-index-september-2025-report</guid><category>News</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[News] Follow AnthropicAnthropic on XLinkedIn</title><link>https://x.com/AnthropicAI</link><description>Latest from Anthropic: Follow AnthropicAnthropic on XLinkedIn (Source: News)</description><guid isPermaLink="false">https://x.com/AnthropicAI</guid><category>News</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[News] Media assetsDownload press kit</title><link>https://anthropic.com/press-kit</link><description>Latest from Anthropic: Media assetsDownload press kit (Source: News)</description><guid isPermaLink="false">https://anthropic.com/press-kit</guid><category>News</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[News] Anthropic officially opens Tokyo office, signs Memorandum of Cooperation with the Japan AI Safety Institute</title><link>https://www.anthropic.com/news/opening-our-tokyo-office</link><description>AnnouncementsAnthropic officially opens Tokyo office, signs Memorandum of Cooperation with the Japan AI Safety Institute... (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/opening-our-tokyo-office</guid><category>News</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[News] Advancing Claude for Financial Services</title><link>https://www.anthropic.com/news/advancing-claude-for-financial-services</link><description>AnnouncementsAdvancing Claude for Financial Services... (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/advancing-claude-for-financial-services</guid><category>News</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[News] Seoul becomes Anthropic’s third office in Asia-Pacific as we continue our international growth</title><link>https://www.anthropic.com/news/seoul-becomes-third-anthropic-office-in-asia-pacific</link><description>AnnouncementsSeoul becomes Anthropic’s third office in Asia-Pacific as we continue our international growth... (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/seoul-becomes-third-anthropic-office-in-asia-pacific</guid><category>News</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[News] Expanding our use of Google Cloud TPUs and Services</title><link>https://www.anthropic.com/news/expanding-our-use-of-google-cloud-tpus-and-services</link><description>AnnouncementsExpanding our use of Google Cloud TPUs and Services... (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/expanding-our-use-of-google-cloud-tpus-and-services</guid><category>News</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[News] A statement from Dario Amodei on Anthropic's commitment to American AI leadership</title><link>https://www.anthropic.com/news/statement-dario-amodei-american-ai-leadership</link><description>AnnouncementsA statement from Dario Amodei on Anthropic's commitment to American AI leadership... (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/statement-dario-amodei-american-ai-leadership</guid><category>News</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[News] Claude Code on the web</title><link>https://www.anthropic.com/news/claude-code-on-the-web</link><description>ProductClaude Code on the web... (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/claude-code-on-the-web</guid><category>News</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[News] Claude for Life Sciences</title><link>https://www.anthropic.com/news/claude-for-life-sciences</link><description>AnnouncementsClaude for Life Sciences... (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/claude-for-life-sciences</guid><category>News</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[News] Claude and your productivity platforms</title><link>https://www.anthropic.com/news/productivity-platforms</link><description>ProductClaude and your productivity platforms... (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/productivity-platforms</guid><category>News</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[News] Introducing Agent Skills</title><link>https://www.anthropic.com/news/skills</link><description>ProductIntroducing Agent Skills... (Source: News)</description><guid isPermaLink="false">https://www.anthropic.com/news/skills</guid><category>News</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Engineering] Effective context engineering for AI agents</title><link>https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents</link><description>Engineering insights: Effective context engineering for AI agentsSep 29, 2025... (Source: Engineering)</description><guid isPermaLink="false">https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents</guid><category>Engineering</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Engineering] Desktop Extensions: One-click MCP server installation for Claude Desktop</title><link>https://www.anthropic.com/engineering/desktop-extensions</link><description>Engineering insights: Desktop Extensions: One-click MCP server installation for Claude DesktopJun 26, 2025... (Source: Engineering)</description><guid isPermaLink="false">https://www.anthropic.com/engineering/desktop-extensions</guid><category>Engineering</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Engineering] How we built our multi-agent research system</title><link>https://www.anthropic.com/engineering/multi-agent-research-system</link><description>Engineering insights: How we built our multi-agent research systemJun 13, 2025... (Source: Engineering)</description><guid isPermaLink="false">https://www.anthropic.com/engineering/multi-agent-research-system</guid><category>Engineering</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Engineering] Claude Code: Best practices for agentic coding</title><link>https://www.anthropic.com/engineering/claude-code-best-practices</link><description>Engineering insights: Claude Code: Best practices for agentic codingApr 18, 2025... (Source: Engineering)</description><guid isPermaLink="false">https://www.anthropic.com/engineering/claude-code-best-practices</guid><category>Engineering</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Engineering] Engineering at Anthropic: Inside the team building reliable AI systems</title><link>https://console.anthropic.com/</link><description>Engineering insights from Anthropic: Engineering at Anthropic: Inside the team building reliable AI systems (Source: Engineering)</description><guid isPermaLink="false">https://console.anthropic.com/</guid><category>Engineering</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Alignment Science] Reasoning Models Don't Always Say What They Think</title><link>https://www.anthropic.com/research/reasoning-models-dont-say-think</link><description>AI Safety research: We find that reasoning models don't always accurately verbalize their reasoning. This casts doubt on
                   ... (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research/reasoning-models-dont-say-think</guid><category>Alignment Science</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Alignment Science] Alignment Faking Revisited: Improved Classifiers and Open Source Extensions</title><link>https://alignment.anthropic.com/2025/alignment-faking-revisited/</link><description>AI Safety research: We present a replication and extension of an alignment faking model organism.... (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/alignment-faking-revisited/</guid><category>Alignment Science</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Alignment Science] Auditing Language Models for Hidden Objectives</title><link>https://www.anthropic.com/research/auditing-hidden-objectives</link><description>AI Safety research: We deliberately train a language model with a hidden objective and use it as a testbed for studying
                    ... (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research/auditing-hidden-objectives</guid><category>Alignment Science</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Alignment Science] Automated Researchers Can Subtly Sandbag</title><link>https://alignment.anthropic.com/2025/automated-researchers-sandbag/</link><description>AI Safety research: Current models can sandbag ML experiments and research decisions without being detected by zero-shot
                   ... (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/automated-researchers-sandbag/</guid><category>Alignment Science</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Alignment Science] Introducing Anthropic's Safeguards Research Team</title><link>https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html</link><description>AI Safety research: We're launching a new research team focused on mitigating the post-deployment risks of AI systems.... (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html</guid><category>Alignment Science</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Alignment Science] Forecasting Rare Language Model Behaviors</title><link>https://www.anthropic.com/research/forecasting-rare-behaviors</link><description>AI Safety research: We forecast whether risks will occur after a model is deployed — using even very limited sets of
                    tes... (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research/forecasting-rare-behaviors</guid><category>Alignment Science</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Alignment Science] Alignment Faking in Large Language Models</title><link>https://www.anthropic.com/research/alignment-faking</link><description>AI Safety research: We present experiments where Claude often pretends to
                    have different views during training, while ac... (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research/alignment-faking</guid><category>Alignment Science</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Alignment Science] How to Replicate and Extend our Alignment Faking Demo</title><link>https://alignment.anthropic.com/2024/how-to-alignment-faking/index.html</link><description>AI Safety research: We describe how to get started with experimenting with
                    our demonstration of alignment faking, and pr... (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2024/how-to-alignment-faking/index.html</guid><category>Alignment Science</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Alignment Science] Sabotage Evaluations for Frontier Models</title><link>https://www.anthropic.com/research/sabotage-evaluations</link><description>AI Safety research: How well could AI models mislead us, or secretly
                    sabotage tasks, if they were trying to? We describe... (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research/sabotage-evaluations</guid><category>Alignment Science</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Alignment Science] Simple Probes can Catch Sleeper Agents</title><link>https://www.anthropic.com/research/probes-catch-sleeper-agents</link><description>AI Safety research: We find that probing, a simple interpretability
                    technique, can detect when backdoored "sleeper agent... (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research/probes-catch-sleeper-agents</guid><category>Alignment Science</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Alignment Science] Sleeper Agents: Training Deceptive LLMs that Persist
                    Through Safety Training</title><link>https://www.anthropic.com/research/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training</link><description>AI Safety research: We train LLMs to act secretly malicious. We find that,
                    despite our best efforts at alignment trainin... (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training</guid><category>Alignment Science</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Alignment Science] Discovering Language Model Behaviors with Model-Written
                    Evaluations</title><link>https://www.anthropic.com/research/discovering-language-model-behaviors-with-model-written-evaluations</link><description>AI Safety research: We develop an automated way to generate language model
                    (LM) evaluations with LMs, significantly redu... (Source: Alignment Science)</description><guid isPermaLink="false">https://www.anthropic.com/research/discovering-language-model-behaviors-with-model-written-evaluations</guid><category>Alignment Science</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item><item><title>[Alignment Science] Alignment Science Blog</title><link>https://alignment.anthropic.com/2025/sabotage-risk-report/</link><description>AI Safety research: We release a report on the level of risk posed by our deployed models from emerging forms of
                    misalig... (Source: Alignment Science)</description><guid isPermaLink="false">https://alignment.anthropic.com/2025/sabotage-risk-report/</guid><category>Alignment Science</category><pubDate>Thu, 30 Oct 2025 01:50:43 +0000</pubDate></item></channel></rss>