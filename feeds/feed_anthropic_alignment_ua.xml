<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Anthropic Безпека ШІ - AI-PULSE</title>
    <link>https://alignment.anthropic.com</link>
    <description>Дослідження безпеки ШІ та alignment від Anthropic (українською мовою)</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>AI-PULSE RSS Generator - Ukrainian Alignment Science Translation</generator>
    <language>uk</language>
    <lastBuildDate>Wed, 02 Jul 2025 07:59:37 +0000</lastBuildDate>
    <item>
      <title>[Безпека ШІ] Reasoning Models Don't Always Say What They Think</title>
      <link>https://www.anthropic.com/research/reasoning-models-dont-say-think</link>
      <description>Дослідження безпеки ШІ від Anthropic: Reasoning Models Don't Always Say What They Think</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/reasoning-models-dont-say-think</guid>
      <category>Alignment Science</category>
      <pubDate>Wed, 02 Jul 2025 07:59:37 +0000</pubDate>
    </item>
    <item>
      <title>[Безпека ШІ] Alignment Faking Revisited: Improved Classifiers and Open Source Extensions</title>
      <link>https://alignment.anthropic.com/2025/alignment-faking-revisited/</link>
      <description>Дослідження безпеки ШІ від Anthropic: Alignment Faking Revisited: Improved Classifiers and Open Source Extensions</description>
      <guid isPermaLink="false">https://alignment.anthropic.com/2025/alignment-faking-revisited/</guid>
      <category>Alignment Science</category>
      <pubDate>Wed, 02 Jul 2025 07:59:37 +0000</pubDate>
    </item>
    <item>
      <title>[Безпека ШІ] Auditing мовні моделі for Hidden Objectives</title>
      <link>https://www.anthropic.com/research/auditing-hidden-objectives</link>
      <description>Дослідження безпеки ШІ від Anthropic: Auditing мовні моделі for Hidden Objectives</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/auditing-hidden-objectives</guid>
      <category>Alignment Science</category>
      <pubDate>Wed, 02 Jul 2025 07:59:37 +0000</pubDate>
    </item>
    <item>
      <title>[Безпека ШІ] Automated Researchers Can Subtly Sandbag</title>
      <link>https://alignment.anthropic.com/2025/automated-researchers-sandbag/</link>
      <description>Дослідження безпеки ШІ від Anthropic: Automated Researchers Can Subtly Sandbag</description>
      <guid isPermaLink="false">https://alignment.anthropic.com/2025/automated-researchers-sandbag/</guid>
      <category>Alignment Science</category>
      <pubDate>Wed, 02 Jul 2025 07:59:37 +0000</pubDate>
    </item>
    <item>
      <title>[Безпека ШІ] Представляємо Anthropic's Safeguards Research Team</title>
      <link>https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html</link>
      <description>Дослідження безпеки ШІ від Anthropic: Представляємо Anthropic's Safeguards Research Team</description>
      <guid isPermaLink="false">https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html</guid>
      <category>Alignment Science</category>
      <pubDate>Wed, 02 Jul 2025 07:59:37 +0000</pubDate>
    </item>
    <item>
      <title>[Безпека ШІ] Forecasting Rare Language Model Behaviors</title>
      <link>https://www.anthropic.com/research/forecasting-rare-behaviors</link>
      <description>Дослідження безпеки ШІ від Anthropic: Forecasting Rare Language Model Behaviors</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/forecasting-rare-behaviors</guid>
      <category>Alignment Science</category>
      <pubDate>Wed, 02 Jul 2025 07:59:37 +0000</pubDate>
    </item>
    <item>
      <title>[Безпека ШІ] Alignment Faking in Large мовні моделі</title>
      <link>https://www.anthropic.com/research/alignment-faking</link>
      <description>Дослідження безпеки ШІ від Anthropic: Alignment Faking in Large мовні моделі</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/alignment-faking</guid>
      <category>Alignment Science</category>
      <pubDate>Wed, 02 Jul 2025 07:59:37 +0000</pubDate>
    </item>
    <item>
      <title>[Безпека ШІ] How to Replicate and Extend our Alignment Faking Demo</title>
      <link>https://alignment.anthropic.com/2024/how-to-alignment-faking/index.html</link>
      <description>Дослідження безпеки ШІ від Anthropic: How to Replicate and Extend our Alignment Faking Demo</description>
      <guid isPermaLink="false">https://alignment.anthropic.com/2024/how-to-alignment-faking/index.html</guid>
      <category>Alignment Science</category>
      <pubDate>Wed, 02 Jul 2025 07:59:37 +0000</pubDate>
    </item>
    <item>
      <title>[Безпека ШІ] Sabotage Eоцінкаs for Frontier Models</title>
      <link>https://www.anthropic.com/research/sabotage-evaluations</link>
      <description>Дослідження безпеки ШІ від Anthropic: Sabotage Eоцінкаs for Frontier Models</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/sabotage-evaluations</guid>
      <category>Alignment Science</category>
      <pubDate>Wed, 02 Jul 2025 07:59:37 +0000</pubDate>
    </item>
    <item>
      <title>[Безпека ШІ] Simple Probes can Catch Sleeper Agents</title>
      <link>https://www.anthropic.com/research/probes-catch-sleeper-agents</link>
      <description>Дослідження безпеки ШІ від Anthropic: Simple Probes can Catch Sleeper Agents</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/probes-catch-sleeper-agents</guid>
      <category>Alignment Science</category>
      <pubDate>Wed, 02 Jul 2025 07:59:37 +0000</pubDate>
    </item>
    <item>
      <title>[Безпека ШІ] Sleeper Agents: Training Deceptive LLMs that Persist
                    Through Safety Training</title>
      <link>https://www.anthropic.com/research/many-shot-jailbreaking</link>
      <description>Дослідження безпеки ШІ від Anthropic: Sleeper Agents: Training Deceptive LLMs that Persist
                    Through Safety Training</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/many-shot-jailbreaking</guid>
      <category>Alignment Science</category>
      <pubDate>Wed, 02 Jul 2025 07:59:37 +0000</pubDate>
    </item>
    <item>
      <title>[Безпека ШІ] Discovering Language Model Behaviors with Model-Written
                    Eоцінкаs</title>
      <link>https://www.anthropic.com/research/discovering-language-model-behaviors-with-model-written-evaluations</link>
      <description>Дослідження безпеки ШІ від Anthropic: Discovering Language Model Behaviors with Model-Written
                    Eоцінкаs</description>
      <guid isPermaLink="false">https://www.anthropic.com/research/discovering-language-model-behaviors-with-model-written-evaluations</guid>
      <category>Alignment Science</category>
      <pubDate>Wed, 02 Jul 2025 07:59:37 +0000</pubDate>
    </item>
    <item>
      <title>[Безпека ШІ] Alignment Science Blog</title>
      <link>https://alignment.anthropic.com/2025/cheap-monitors/</link>
      <description>Дослідження безпеки ШІ від Anthropic: Alignment Science Blog</description>
      <guid isPermaLink="false">https://alignment.anthropic.com/2025/cheap-monitors/</guid>
      <category>Alignment Science</category>
      <pubDate>Wed, 02 Jul 2025 07:59:37 +0000</pubDate>
    </item>
  </channel>
</rss>
